{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2335d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# Add project root to Python path (go up one level from notebooks/)\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    \n",
    "import torch\n",
    "from model.model_loader import ModelLoader\n",
    "from main import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e4ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    \"\"\"Quick test to verify model loads and runs.\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    config = load_config('configs/config.yaml')\n",
    "    \n",
    "    # Initialize model loader\n",
    "    model_loader = ModelLoader()\n",
    "    \n",
    "    # Load tokenizer ONCE (it's the same for all strategies)\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model']['distilbert'])\n",
    "    \n",
    "    # Test different freeze strategies\n",
    "    print(\"\\n=== Testing Model Loading ===\")\n",
    "    for strategy in ['frozen', 'partial', 'full']:\n",
    "        print(f\"\\n--- Strategy: {strategy} ---\")\n",
    "        model, _ = model_loader.load_model_and_tokenizer(freeze_strategy=strategy)\n",
    "        \n",
    "        # Count parameters\n",
    "        total, trainable = model_loader.count_parameters(model)\n",
    "        print(f\"Total parameters: {total:,}\")\n",
    "        print(f\"Trainable parameters: {trainable:,}\")\n",
    "        print(f\"Trainable percentage: {100 * trainable / total:.2f}%\")\n",
    "    \n",
    "    # Use 'frozen' for quick test\n",
    "    print(\"\\n=== Testing Forward Pass (Untrained) ===\")\n",
    "    model, _ = model_loader.load_model_and_tokenizer(freeze_strategy='frozen')  # Reuse tokenizer\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dummy input\n",
    "    test_texts = [\n",
    "        \"Apple stock rises on strong earnings\",\n",
    "        \"Market crashes due to inflation fears\",\n",
    "        \"Stock remains stable despite news\"\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    max_length = config['model']['max_len']  # Get from config\n",
    "    inputs = tokenizer(\n",
    "        test_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    print(\"\\nPredictions (before training):\")\n",
    "    class_names = ['up', 'stable', 'down']\n",
    "    for text, pred, logit in zip(test_texts, predictions, logits):\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"Prediction: {class_names[pred.item()]}\")\n",
    "        print(f\"Logits: {logit.tolist()}\")\n",
    "        print()\n",
    "    \n",
    "    # Quick training test\n",
    "    print(\"\\n=== Testing Training Step ===\")\n",
    "    lr = config['training']['learning_rate']\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Dummy labels\n",
    "    labels = torch.tensor([0, 2, 1])  # up, down, stable\n",
    "    \n",
    "    # Single training step\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Loss after 1 step: {loss.item():.4f}\")\n",
    "    \n",
    "    # Check if parameters updated\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        new_predictions = torch.argmax(new_logits, dim=1)\n",
    "    \n",
    "    print(\"\\nPredictions (after 1 training step):\")\n",
    "    for text, pred, logit in zip(test_texts, new_predictions, new_logits):\n",
    "        print(f\"Text: {text[:50]}...\")\n",
    "        print(f\"Prediction: {class_names[pred.item()]}\")\n",
    "        print(f\"Logits: {logit.tolist()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceff5c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "\n",
      "=== Testing Model Loading ===\n",
      "\n",
      "--- Strategy: frozen ---\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Froze all transformer layers\n",
      "Total parameters: 66,365,187\n",
      "Trainable parameters: 2,307\n",
      "Trainable percentage: 0.00%\n",
      "\n",
      "--- Strategy: partial ---\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Froze all transformer layers\n",
      "Unfroze top 2 layers (layers [4, 5]) out of 6 total\n",
      "Total parameters: 66,365,187\n",
      "Trainable parameters: 14,178,051\n",
      "Trainable percentage: 21.36%\n",
      "\n",
      "--- Strategy: full ---\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Unfroze all transformer layers\n",
      "Total parameters: 66,365,187\n",
      "Trainable parameters: 66,365,187\n",
      "Trainable percentage: 100.00%\n",
      "\n",
      "=== Testing Forward Pass (Untrained) ===\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Froze all transformer layers\n",
      "\n",
      "Predictions (before training):\n",
      "Text: Apple stock rises on strong earnings...\n",
      "Prediction: up\n",
      "Logits: [0.2763543128967285, 0.17482073605060577, 0.1170700192451477]\n",
      "\n",
      "Text: Market crashes due to inflation fears...\n",
      "Prediction: up\n",
      "Logits: [0.21030032634735107, 0.17155268788337708, 0.044774413108825684]\n",
      "\n",
      "Text: Stock remains stable despite news...\n",
      "Prediction: up\n",
      "Logits: [0.31460636854171753, 0.11511755734682083, 0.11738654971122742]\n",
      "\n",
      "\n",
      "=== Testing Training Step ===\n",
      "Loss after 1 step: 1.1826\n",
      "\n",
      "Predictions (after 1 training step):\n",
      "Text: Apple stock rises on strong earnings...\n",
      "Prediction: up\n",
      "Logits: [0.2884928286075592, 0.15285880863666534, 0.11507894098758698]\n",
      "\n",
      "Text: Market crashes due to inflation fears...\n",
      "Prediction: up\n",
      "Logits: [0.18886920809745789, 0.1509731262922287, 0.0739578902721405]\n",
      "\n",
      "Text: Stock remains stable despite news...\n",
      "Prediction: up\n",
      "Logits: [0.29117465019226074, 0.13867594301700592, 0.10866580903530121]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the test\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55122f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Loaded config from c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\configs\\config.yaml\n",
      "Froze all transformer layers\n",
      "\n",
      "=== Testing Multiple Training Steps ===\n",
      "Step 2: Loss = 1.0399\n",
      "Step 4: Loss = 0.9469\n",
      "Step 6: Loss = 1.1083\n",
      "Step 8: Loss = 0.9792\n",
      "Step 10: Loss = 0.8291\n",
      "\n",
      "Final predictions: ['up', 'down', 'stable']\n",
      "True labels:       ['up', 'down', 'stable']\n",
      "Accuracy: 3/3\n"
     ]
    }
   ],
   "source": [
    "# Add this as a NEW cell after your current test_model() cell\n",
    "\n",
    "def test_extended_training():\n",
    "    \"\"\"Test that model can actually learn from data.\"\"\"\n",
    "    \n",
    "    # Load config\n",
    "    config = load_config('configs/config.yaml')\n",
    "    \n",
    "    # Initialize model loader\n",
    "    model_loader = ModelLoader()\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model']['distilbert'])\n",
    "    model, _ = model_loader.load_model_and_tokenizer(freeze_strategy='frozen')\n",
    "    \n",
    "    # Create dummy input\n",
    "    test_texts = [\n",
    "        \"Apple stock rises on strong earnings\",\n",
    "        \"Market crashes due to inflation fears\",\n",
    "        \"Stock remains stable despite news\"\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    max_length = config['model']['max_len']\n",
    "    inputs = tokenizer(\n",
    "        test_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Setup training\n",
    "    lr = config['training']['learning_rate']\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    labels = torch.tensor([0, 2, 1])  # up, down, stable\n",
    "    \n",
    "    # Train for multiple steps\n",
    "    print(\"\\n=== Testing Multiple Training Steps ===\")\n",
    "    for step in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % 2 == 0:  # Print every 2 steps\n",
    "            print(f\"Step {step+1}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    # Check final predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_logits = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "        final_preds = torch.argmax(final_logits, dim=1)\n",
    "    \n",
    "    print(f\"\\nFinal predictions: {[['up', 'stable', 'down'][p] for p in final_preds.tolist()]}\")\n",
    "    print(f\"True labels:       {[['up', 'stable', 'down'][l] for l in labels.tolist()]}\")\n",
    "    print(f\"Accuracy: {(final_preds == labels).sum().item()}/{len(labels)}\")\n",
    "\n",
    "# Run it\n",
    "test_extended_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
