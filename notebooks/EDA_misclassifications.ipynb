{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfd8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b80d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project_root: C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_project_root(start: Path = None, markers=(\".git\", \"pyproject.toml\", \"setup.py\")) -> Path:\n",
    "    start = Path(start or Path.cwd()).resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "    return Path.cwd()\n",
    "\n",
    "# Prefer explicit env var for cloud runs; otherwise try __file__ then cwd then repo-root\n",
    "project_root = None\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    project_root = Path(os.environ[\"PROJECT_ROOT\"]).resolve()\n",
    "else:\n",
    "    try:\n",
    "        # works in scripts, not in notebooks\n",
    "        project_root = Path(__file__).parent.resolve()\n",
    "    except NameError:\n",
    "        # notebook / interactive fallback: try repo root then cwd\n",
    "        project_root = find_project_root()\n",
    "\n",
    "# build paths relative to project_root\n",
    "notebooks_dir = project_root / \"notebooks\"\n",
    "mis_dir = notebooks_dir / \"misclassifications\"\n",
    "\n",
    "bert_path = mis_dir / \"misclassifications_Stanford_bert_partial_finetune.csv\"\n",
    "bert_lora_path = mis_dir / \"misclassifications_Stanford_bert_lora_r8.csv\"\n",
    "distilled_bert_path = mis_dir / \"misclassifications_Stanford_distilbert_lora_r8.csv\"\n",
    "distilled_bert_lora_path = mis_dir / \"misclassifications_Stanford_distilbert_partial_finetune.csv\"\n",
    "\n",
    "print(\"Using project_root:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b09fd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\n",
      "mis_dir: C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\notebooks\\misclassifications\n",
      "expected bert_path: C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\notebooks\\misclassifications\\misclassifications_Stanford_bert_partial_finetune.csv\n",
      "bert_path.exists(): False\n",
      "Searching repo for misclassifications_Stanford_bert_partial_finetune.csv ...\n",
      "Found bert_path at: C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\misclassifications\\misclassifications_Stanford_bert_partial_finetune.csv\n",
      "Loading misclassifications_Stanford_bert_lora_r8.csv from C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\misclassifications\\misclassifications_Stanford_bert_lora_r8.csv\n",
      "Loading misclassifications_Stanford_distilbert_lora_r8.csv from C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\misclassifications\\misclassifications_Stanford_distilbert_lora_r8.csv\n",
      "Loading misclassifications_Stanford_distilbert_partial_finetune.csv from C:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\misclassifications\\misclassifications_Stanford_distilbert_partial_finetune.csv\n",
      "Loaded dataframes: {'bert': 14078, 'bert_lora': 16076, 'distilled': 18512, 'distilled_lora': 17063}\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "from pathlib import Path\n",
    "\n",
    "def find_file(name, start=project_root):\n",
    "    start = Path(start or Path.cwd()).resolve()\n",
    "    return list(start.rglob(name))\n",
    "\n",
    "print(\"project_root:\", project_root)\n",
    "print(\"mis_dir:\", mis_dir)\n",
    "print(\"expected bert_path:\", bert_path)\n",
    "print(\"bert_path.exists():\", bert_path.exists())\n",
    "\n",
    "if not bert_path.exists():\n",
    "    print(\"Searching repo for misclassifications_Stanford_bert_partial_finetune.csv ...\")\n",
    "    found = find_file(\"misclassifications_Stanford_bert_partial_finetune.csv\", project_root)\n",
    "    if not found:\n",
    "        raise FileNotFoundError(f\"Could not locate misclassifications_Stanford_bert_partial_finetune.csv under {project_root}\")\n",
    "    bert_path = found[0]\n",
    "    print(\"Found bert_path at:\", bert_path)\n",
    "\n",
    "# helper to load or search\n",
    "def load_or_search(p: Path):\n",
    "    if isinstance(p, str):\n",
    "        p = Path(p)\n",
    "    if p.exists():\n",
    "        return pd.read_csv(p)\n",
    "    found = find_file(p.name, project_root)\n",
    "    if not found:\n",
    "        raise FileNotFoundError(f\"Could not locate {p.name} under {project_root}\")\n",
    "    print(f\"Loading {p.name} from {found[0]}\")\n",
    "    return pd.read_csv(found[0])\n",
    "\n",
    "df_bert = load_or_search(bert_path)\n",
    "df_bert_lora = load_or_search(bert_lora_path)\n",
    "df_distilled_bert = load_or_search(distilled_bert_path)\n",
    "df_distilled_bert_lora = load_or_search(distilled_bert_lora_path)\n",
    "\n",
    "print(\"Loaded dataframes:\", { 'bert': len(df_bert), 'bert_lora': len(df_bert_lora),\n",
    "                             'distilled': len(df_distilled_bert), 'distilled_lora': len(df_distilled_bert_lora) })\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2128c9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert: rows=14078 text_col=text true_col=true_label pred_col=predicted_label misclassified=14078\n",
      "bert_lora: rows=16076 text_col=text true_col=true_label pred_col=predicted_label misclassified=16076\n",
      "distilled: rows=18512 text_col=text true_col=true_label pred_col=predicted_label misclassified=18512\n",
      "distilled_lora: rows=17063 text_col=text true_col=true_label pred_col=predicted_label misclassified=17063\n"
     ]
    }
   ],
   "source": [
    "# 1) Inspect dataframes to detect text / gold / prediction columns and miscounts\n",
    "def infer_cols(df):\n",
    "    cols = [c for c in df.columns.tolist()]\n",
    "    text_col = next((c for c in cols if 'text' in c.lower() or 'sentence' in c.lower() or 'tweet' in c.lower()), None)\n",
    "    true_col = next((c for c in cols if c.lower() in ['label','labels','gold','true_label','target','y_true','y']), None)\n",
    "    pred_col = next((c for c in cols if 'pred' in c.lower() or 'prediction' in c.lower() or 'predicted' in c.lower()), None)\n",
    "    # fallback heuristics\n",
    "    if text_col is None:\n",
    "        text_col = next((c for c in cols if df[c].dtype == object and df[c].str.len().mean() > 10), None)\n",
    "    return text_col, true_col, pred_col\n",
    "\n",
    "frames = {'bert': df_bert, 'bert_lora': df_bert_lora, 'distilled': df_distilled_bert, 'distilled_lora': df_distilled_bert_lora}\n",
    "colinfo = {}\n",
    "for name, df in frames.items():\n",
    "    t, y, p = infer_cols(df)\n",
    "    miscount = None\n",
    "    if y and p:\n",
    "        miscount = int((df[y] != df[p]).sum())\n",
    "    print(f\"{name}: rows={len(df)} text_col={t} true_col={y} pred_col={p} misclassified={miscount}\")\n",
    "    colinfo[name] = (t, y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4695e512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.11\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "459510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 86, in setup_cli\n",
      "    command = get_command(app)\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 350, in get_command\n",
      "    click_command: click.Command = get_group(typer_instance)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 332, in get_group\n",
      "    group = get_group_from_info(\n",
      "            ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 483, in get_group_from_info\n",
      "    command = get_command_from_info(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 577, in get_command_from_info\n",
      "    ) = get_params_convertors_ctx_param_name_from_function(command_info.callback)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 553, in get_params_convertors_ctx_param_name_from_function\n",
      "    click_param, convertor = get_click_param(param)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\main.py\", line 877, in get_click_param\n",
      "    TyperOption(\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\typer\\core.py\", line 498, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"c:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\click\\core.py\", line 2793, in __init__\n",
      "    raise TypeError(\"Secondary flag is not valid for non-boolean flag.\")\n",
      "TypeError: Secondary flag is not valid for non-boolean flag.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40609169",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57aee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Feature extraction focusing on sentence structure and syntax\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def syntax_features(text):\n",
    "    doc = nlp(text if isinstance(text, str) else '')\n",
    "    tok_texts = [t.text for t in doc]\n",
    "    tokens = [t for t in doc]\n",
    "    char_len = len(text) if text else 0\n",
    "    token_count = len(tokens)\n",
    "    avg_token_len = np.mean([len(t.text) for t in tokens]) if tokens else 0\n",
    "    punct_count = sum(1 for ch in text if ch in string.punctuation)\n",
    "    comma_count = text.count(',') if text else 0\n",
    "    uppercase_ratio = np.mean([1 if (t.text.isupper() and t.text.isalpha()) else 0 for t in tokens]) if tokens else 0\n",
    "    digit_ratio = np.mean([1 if any(ch.isdigit() for ch in t.text) else 0 for t in tokens]) if tokens else 0\n",
    "    negations = sum(1 for t in tokens if t.dep_ == 'neg' or t.lemma_.lower() in {'not','no','never'})\n",
    "    ner_count = len(doc.ents)\n",
    "    # approximate clause count by counting certain dependency labels\n",
    "    clause_deps = {'advcl','ccomp','xcomp','acl','relcl'}\n",
    "    clause_count = sum(1 for t in tokens if t.dep_ in clause_deps)\n",
    "    # tree depth: maximum ancestor chain length\n",
    "    def token_depth(tok):\n",
    "        depth = 0\n",
    "        cur = tok\n",
    "        while cur.head is not cur:\n",
    "            depth += 1\n",
    "            cur = cur.head\n",
    "            if depth > 200:\n",
    "                break\n",
    "        return depth\n",
    "    depth_vals = [token_depth(t) for t in tokens] if tokens else [0]\n",
    "    max_depth = int(max(depth_vals)) if depth_vals else 0\n",
    "    # POS distribution (top 5 tags)\n",
    "    pos_counts = Counter([t.pos_ for t in tokens])\n",
    "    pos_top = dict(pos_counts.most_common(5))\n",
    "    return {\n",
    "        'char_len': char_len,\n",
    "        'token_count': token_count,\n",
    "        'avg_token_len': float(avg_token_len),\n",
    "        'punct_count': punct_count,\n",
    "        'comma_count': comma_count,\n",
    "        'uppercase_ratio': float(uppercase_ratio),\n",
    "        'digit_ratio': float(digit_ratio),\n",
    "        'negation_count': negations,\n",
    "        'ner_count': ner_count,\n",
    "        'clause_count': int(clause_count),\n",
    "        'max_dep_depth': int(max_depth),\n",
    "        'pos_top': pos_top\n",
    "    }\n",
    "\n",
    "def annotate_df(df, text_col, true_col, pred_col, max_rows=None):\n",
    "    sub = df if max_rows is None else df.head(max_rows).copy()\n",
    "    sub = sub.copy()\n",
    "    sub['is_mis'] = False if (true_col is None or pred_col is None) else (sub[true_col] != sub[pred_col])\n",
    "    feats = []\n",
    "    for i, txt in enumerate(sub[text_col].fillna('').astype(str)):\n",
    "        feats.append(syntax_features(txt))\n",
    "    feats_df = pd.DataFrame(feats)\n",
    "    # expand pos_top keys into columns (sparse)\n",
    "    pos_df = feats_df['pos_top'].apply(lambda d: pd.Series(d)).fillna(0).astype(int)\n",
    "    feats_df = pd.concat([feats_df.drop(columns=['pos_top']), pos_df], axis=1)\n",
    "    res = pd.concat([sub.reset_index(drop=True), feats_df.reset_index(drop=True)], axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56ebf2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating bert (this may take a bit) - using columns text=text true=true_label pred=predicted_label\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnnotating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (this may take a bit) - using columns text=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m true=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mycol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pred=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     annotated[name] = \u001b[43mannotate_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# adjust max_rows as needed\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnnotated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(annotated[name])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Example aggregated comparison plots: token_count, max_dep_depth, clause_count, ner_count, punct_count\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mannotate_df\u001b[39m\u001b[34m(df, text_col, true_col, pred_col, max_rows)\u001b[39m\n\u001b[32m     55\u001b[39m feats = []\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, txt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sub[text_col].fillna(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     feats.append(\u001b[43msyntax_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m feats_df = pd.DataFrame(feats)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# expand pos_top keys into columns (sparse)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36msyntax_features\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msyntax_features\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     tok_texts = [t.text \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[32m      8\u001b[39m     tokens = [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.predict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:33\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(model, X, is_train):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     step_model = \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munseen_classes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhas_upper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model.finish_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[39m, in \u001b[36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py:76\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     74\u001b[39m pad = model.attrs[\u001b[33m\"\u001b[39m\u001b[33mpad\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m Xf = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m Yf, get_dXf = layer(Xf, is_train)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Besitzer\\OneDrive\\Dokumente\\CBS_Copenhagen\\Semester\\WS2025\\AdvNLP\\Final Exam\\AVDNLP_final_project\\venv\\Lib\\site-packages\\thinc\\backends\\ops.py:337\u001b[39m, in \u001b[36mOps.flatten\u001b[39m\u001b[34m(self, X, dtype, pad, ndim_if_empty)\u001b[39m\n\u001b[32m    335\u001b[39m padded = []\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     padded.append(xp.zeros((pad,) + x.shape[\u001b[32m1\u001b[39m:], dtype=x.dtype))\n\u001b[32m    338\u001b[39m     padded.append(x)\n\u001b[32m    339\u001b[39m padded.append(xp.zeros((pad,) + x.shape[\u001b[32m1\u001b[39m:], dtype=x.dtype))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 4) Annotate each dataframe (limit rows if needed for speed) and compare distributions\n",
    "annotated = {}\n",
    "for name, df in frames.items():\n",
    "    tcol, ycol, pcol = colinfo.get(name, (None, None, None))\n",
    "    if tcol is None:\n",
    "        print(f\"Skipping {name}: no text column detected\")\n",
    "        continue\n",
    "    print(f\"Annotating {name} (this may take a bit) - using columns text={tcol} true={ycol} pred={pcol}\")\n",
    "    annotated[name] = annotate_df(df, tcol, ycol, pcol, max_rows=2000)  # adjust max_rows as needed\n",
    "    print(f\"Annotated {name}: {len(annotated[name])} rows\")\n",
    "\n",
    "# Example aggregated comparison plots: token_count, max_dep_depth, clause_count, ner_count, punct_count\n",
    "keys = ['token_count','max_dep_depth','clause_count','ner_count','punct_count','negation_count','uppercase_ratio']\n",
    "for name, ann in annotated.items():\n",
    "    if 'is_mis' not in ann.columns:\n",
    "        continue\n",
    "    print('\\nModel:', name)\n",
    "    display_cols = [k for k in keys if k in ann.columns]\n",
    "    fig, axes = plt.subplots(len(display_cols), 1, figsize=(8, 3*len(display_cols)))\n",
    "    if len(display_cols) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, col in zip(axes, display_cols):\n",
    "        sns.boxplot(x='is_mis', y=col, data=ann, ax=ax)\n",
    "        ax.set_title(f\"{name} â€” {col} by misclassified\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) POS differences and top tokens in misclassified vs correct\n",
    "for name, ann in annotated.items():\n",
    "    if 'is_mis' not in ann.columns:\n",
    "        continue\n",
    "    print('\\nPOS / token-level diff for', name)\n",
    "    pos_cols = [c for c in ann.columns if c.isupper() and len(c) <= 5]  # heuristic for POS columns created\n",
    "    # fallback: compute POS counts from spaCy on a sample\n",
    "    sample = ann.sample(min(500, len(ann)), random_state=1)\n",
    "    # compute simple token frequency separately\n",
    "    mis_tokens = Counter()\n",
    "    ok_tokens = Counter()\n",
    "    for _, row in sample.iterrows():\n",
    "        doc = nlp(str(row[colinfo[name][0]])) if colinfo[name][0] else nlp('')\n",
    "        toks = [t.lemma_.lower() for t in doc if t.is_alpha]\n",
    "        if row.get('is_mis', False):\n",
    "            mis_tokens.update(toks)\n",
    "        else:\n",
    "            ok_tokens.update(toks)\n",
    "    # top differences\n",
    "    top_mis = {k: mis_tokens[k] for k in list(dict(mis_tokens.most_common(20)).keys())}\n",
    "    top_ok = {k: ok_tokens[k] for k in list(dict(ok_tokens.most_common(20)).keys())}\n",
    "    print('Top tokens in misclassified (sample):', list(top_mis.items())[:10])\n",
    "    print('Top tokens in correct (sample):', list(top_ok.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Show representative misclassified examples to inspect syntactic issues\n",
    "for name, ann in annotated.items():\n",
    "    if 'is_mis' not in ann.columns:\n",
    "        continue\n",
    "    print('\\n=== Model:', name, 'sample misclassified examples ===')\n",
    "    # prioritize long/deep sentences and many clauses/punctuation\n",
    "    mis = ann[ann['is_mis']].sort_values(by=['max_dep_depth','clause_count','token_count'], ascending=False).head(10)\n",
    "    for i, row in mis.iterrows():\n",
    "        txt = row[colinfo[name][0]] if colinfo[name][0] in row else row.get(colinfo[name][0], '')\n",
    "        print('---')\n",
    "        print('index:', i)\n",
    "        print('true:', row[colinfo[name][1]] if colinfo[name][1] in row else None, 'pred:', row[colinfo[name][2]] if colinfo[name][2] in row else None)\n",
    "        print('token_count:', row.get('token_count'), 'max_dep_depth:', row.get('max_dep_depth'), 'clause_count:', row.get('clause_count'), 'neg:', row.get('negation_count'))\n",
    "        print('text:', txt)\n",
    "        # small parse printing to visualize structure\n",
    "        doc = nlp(str(txt))\n",
    "        print('POS tags:', ' '.join(f\"{t.text}/{t.pos_}\" for t in doc))\n",
    "        print()\n",
    "\n",
    "    # Also show short ambiguous examples\n",
    "    short_mis = ann[ann['is_mis']].sort_values(by='token_count', ascending=True).head(5)\n",
    "    if len(short_mis):\n",
    "        print('\\nShort misclassified examples:')\n",
    "        for _, row in short_mis.iterrows():\n",
    "            print('-', row[colinfo[name][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24635f",
   "metadata": {},
   "source": [
    "Notes / next steps:\n",
    "- Inspect printed examples for recurring structural issues: long sentences with many clauses, heavy punctuation, nested clauses, or negation patterns.\n",
    "- If patterns appear (e.g. many negations or long dependency chains), consider targeted data augmentation or model fine-tuning with syntactic-aware objectives.\n",
    "- You can increase annotate_df max_rows or run on full datasets if compute/time allows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
