# DistilBERT with Partial Fine-Tuning  
# Unfreeze only the top N transformer layers + classifier head
# DistilBERT has 6 layers total (vs BERT's 12)

# General settings
seed: 42

# Data settings
data:
  file_path: "Stanford/snli_1.0_train.txt"  # relative path inside the dataset or empty for default
  data_dir: "Stanford"  # relative to project root
  test_size: 0.2
  val_size: 0.1
  pandas_kwargs: {} # Pandas settings for reading CSV files
  label_map: {'entailment': 0, 'neutral': 1, 'contradiction': 2}

# Weights & Biases configuration
wandb:
  project: "NLP_mini"
  entity: "jojs-it-universitetet-i-k-benhavn"
  run_name: "Stanford_distilbert_partial_finetune"
  tags: ['Stanford','distilbert', 'partial-finetune']
  log_model: false

# Model settings
model:
  name: "distilbert-base-uncased"
  model_type: "distilbert"
  use_lora: false
  freeze_strategy: "partial"  # unfreeze top N layers
  num_frozen_layers: 2  # number of top layers to unfreeze (DistilBERT has 6 total)
  max_len: 256
  dropout_rate: 0.5
  checkpoint_dir: "checkpoints"
  save_local: false

# Training settings
training:
  batch_size: 8
  num_epochs: 5
  learning_rate: 0.00002  
  dropout_rate: 0.5
  weight_decay: 0.1
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  early_stopping_patience: 5

